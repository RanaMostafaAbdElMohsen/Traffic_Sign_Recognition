{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LaO6nESd4Sh"
   },
   "outputs": [],
   "source": [
    "# def IntialiseEnv():\n",
    "#     nb_dir = os.path.split(os.getcwd())[0]\n",
    "#     if nb_dir not in sys.path:\n",
    "#         sys.path.append(nb_dir)\n",
    "#     invalid_path='/opt/ros/kinetic/lib/python2.7/dist-packages'\n",
    "#     if invalid_path in sys.path:\n",
    "#         sys.path.remove(invalid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATNb_MiEeZp0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "658eLRI4edFm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/DL_PRJ/Traffic_Sign_Recognition_Detection/Source_Code')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fvl0AvkshIeT",
    "outputId": "59472ff3-4d10-4624-ea75-f76ab272b926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hB2Dgodd4Sn"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "# import import_ipynb\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from Data_Preparation.Data_Preparation import LoadTrainDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrdVj0evjrbE"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import warnings\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "# import import_ipynb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.color\n",
    "import skimage.filters\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "# import import_ipynb\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_20Szs7jrrm"
   },
   "outputs": [],
   "source": [
    "## Sharpen Image\n",
    "def sharpen(img):\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    im = cv2.filter2D(img, -1, kernel)\n",
    "    plt.imshow(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqhRA-8jjrud"
   },
   "outputs": [],
   "source": [
    "# Excessive sharpening image\n",
    "def excessive(img):\n",
    "    kernel = np.array([[1,1,1], [1,-7,1], [1,1,1]])\n",
    "    im = cv2.filter2D(img, -1, kernel)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJlWXtcXjsLv"
   },
   "outputs": [],
   "source": [
    "# Blur of images\n",
    "def blur(img):\n",
    "    blur =  cv2.medianBlur(img,5)\n",
    "    return blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSuNoGo_jsJW"
   },
   "outputs": [],
   "source": [
    "# Edge Enhancement\n",
    "def edgeEnhancement(img):\n",
    "    kernel = np.array([[-1,-1,-1,-1,-1],\n",
    "                               [-1,2,2,2,-1],\n",
    "                               [-1,2,8,2,-1],\n",
    "                               [-2,2,2,2,-1],\n",
    "                               [-1,-1,-1,-1,-1]])/8.0\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    image=cv2.filter2D(img, -1, kernel)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEBrc8VHjsBy"
   },
   "outputs": [],
   "source": [
    "# Weight image with another image\n",
    "def addWeightedImg(img,blur):\n",
    "    result = cv2.addWeighted(img, 1, blur, -0.5, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVtMXeitjr3a"
   },
   "outputs": [],
   "source": [
    "# Contrast Enhancement\n",
    "def ContrastEnhancement():\n",
    "    hsvImg = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    hsvImg[...,1] = hsvImg[...,1]*1.6\n",
    "    hsvImg[...,2] = hsvImg[...,2]*0.8\n",
    "    img=cv2.cvtColor(hsvImg,cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfaz2iNAkeE-"
   },
   "outputs": [],
   "source": [
    "#Edge Detection\n",
    "def CannyEdgeDetection(img):\n",
    "    edged=cv2.Canny(img,60,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXFZINpTkeNe"
   },
   "outputs": [],
   "source": [
    "# Find Contours\n",
    "def FindContours(edged):\n",
    "    contours, hierarchy=cv2.findContours(edged,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.drawContours(img,contours,-1,(0,255,0),1)\n",
    "    ret =40\n",
    "    img[img>ret]=255\n",
    "    img[img<=ret]=0\n",
    "    img =cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSX3RGiLkeVC"
   },
   "outputs": [],
   "source": [
    "def Convert2Grayscale(img):\n",
    "    return skimage.color.rgb2gray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VlSW-QHkeuN"
   },
   "outputs": [],
   "source": [
    "def ThresholdSegmentationOtsu(img):\n",
    "    t = skimage.filters.threshold_otsu(img)\n",
    "    mask = img > t\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmHjUyQfkeqe"
   },
   "outputs": [],
   "source": [
    "def RemoveNoiseColouredImg(img):\n",
    "    dst = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6vblxJckenJ"
   },
   "outputs": [],
   "source": [
    "def HistogramEqualization(img):\n",
    "    img_to_yuv = cv2.cvtColor(img,cv2.COLOR_RGB2YUV)\n",
    "    img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
    "    return cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMoDZpVTkekj"
   },
   "outputs": [],
   "source": [
    "def CalculateContrast(img):\n",
    "    # convert to LAB color space\n",
    "    lab = cv2.cvtColor(img,cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # separate channels\n",
    "    L,A,B=cv2.split(lab)\n",
    "\n",
    "    # compute minimum and maximum in 5x5 region using erode and dilate\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    min = cv2.erode(L,kernel,iterations = 1)\n",
    "    max = cv2.dilate(L,kernel,iterations = 1)\n",
    "\n",
    "    # convert min and max to floats\n",
    "    min = min.astype(np.float64) \n",
    "    max = max.astype(np.float64) \n",
    "\n",
    "    # compute local contrast\n",
    "    contrast = (max-min)/(max+min)\n",
    "\n",
    "    # get average across whole image\n",
    "    average_contrast = 100*np.mean(contrast)\n",
    "\n",
    "    print(str(average_contrast)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vez1vEGJkeia"
   },
   "outputs": [],
   "source": [
    "def from_pil(pimg):\n",
    "    pimg = pimg.convert(mode='RGB')\n",
    "    nimg = np.asarray(pimg)\n",
    "    nimg.flags.writeable = True\n",
    "    return nimg\n",
    "\n",
    "def to_pil(nimg):\n",
    "    return Image.fromarray(np.uint8(nimg))\n",
    "\n",
    "def stretch_pre(nimg):\n",
    "    \"\"\"\n",
    "    from 'Applicability Of White-Balancing Algorithms to Restoring Faded Colour Slides: An Empirical Evaluation'\n",
    "    \"\"\"\n",
    "    nimg = nimg.transpose(2, 0, 1)\n",
    "    nimg[0] = np.maximum(nimg[0]-nimg[0].min(),0)\n",
    "    nimg[1] = np.maximum(nimg[1]-nimg[1].min(),0)\n",
    "    nimg[2] = np.maximum(nimg[2]-nimg[2].min(),0)\n",
    "    return nimg.transpose(1, 2, 0)\n",
    "\n",
    "def grey_world(nimg):\n",
    "    nimg = nimg.transpose(2, 0, 1).astype(np.uint32)\n",
    "    mu_g = np.average(nimg[1])\n",
    "    nimg[0] = np.minimum(nimg[0]*(mu_g/np.average(nimg[0])),255)\n",
    "    nimg[2] = np.minimum(nimg[2]*(mu_g/np.average(nimg[2])),255)\n",
    "    return  nimg.transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "def max_white(nimg):\n",
    "    if nimg.dtype==np.uint8:\n",
    "        brightest=float(2**8)\n",
    "    elif nimg.dtype==np.uint16:\n",
    "        brightest=float(2**16)\n",
    "    elif nimg.dtype==np.uint32:\n",
    "        brightest=float(2**32)\n",
    "    else:\n",
    "        brightest==float(2**8)\n",
    "    nimg = nimg.transpose(2, 0, 1)\n",
    "    nimg = nimg.astype(np.int32)\n",
    "    nimg[0] = np.minimum(nimg[0] * (brightest/float(nimg[0].max())),255)\n",
    "    nimg[1] = np.minimum(nimg[1] * (brightest/float(nimg[1].max())),255)\n",
    "    nimg[2] = np.minimum(nimg[2] * (brightest/float(nimg[2].max())),255)\n",
    "    return nimg.transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "def stretch(nimg):\n",
    "    return max_white(stretch_pre(nimg))\n",
    "\n",
    "def retinex(nimg):\n",
    "    nimg = nimg.transpose(2, 0, 1).astype(np.uint32)\n",
    "    mu_g = nimg[1].max()\n",
    "    nimg[0] = np.minimum(nimg[0]*(mu_g/float(nimg[0].max())),255)\n",
    "    nimg[2] = np.minimum(nimg[2]*(mu_g/float(nimg[2].max())),255)\n",
    "    return nimg.transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "def retinex_adjust(nimg):\n",
    "    \"\"\"\n",
    "    from 'Combining Gray World and Retinex Theory for Automatic White Balance in Digital Photography'\n",
    "    \"\"\"\n",
    "    nimg = nimg.transpose(2, 0, 1).astype(np.uint32)\n",
    "    sum_r = np.sum(nimg[0])\n",
    "    sum_r2 = np.sum(nimg[0]**2)\n",
    "    max_r = nimg[0].max()\n",
    "    max_r2 = max_r**2\n",
    "    sum_g = np.sum(nimg[1])\n",
    "    max_g = nimg[1].max()\n",
    "    coefficient = np.linalg.solve(np.array([[sum_r2,sum_r],[max_r2,max_r]]),\n",
    "                                  np.array([sum_g,max_g]))\n",
    "    nimg[0] = np.minimum((nimg[0]**2)*coefficient[0] + nimg[0]*coefficient[1],255)\n",
    "    sum_b = np.sum(nimg[1])\n",
    "    sum_b2 = np.sum(nimg[1]**2)\n",
    "    max_b = nimg[1].max()\n",
    "    max_b2 = max_r**2\n",
    "    coefficient = np.linalg.solve(np.array([[sum_b2,sum_b],[max_b2,max_b]]),\n",
    "                                             np.array([sum_g,max_g]))\n",
    "    nimg[1] = np.minimum((nimg[1]**2)*coefficient[0] + nimg[1]*coefficient[1],255)\n",
    "    return nimg.transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "def retinex_with_adjust(nimg):\n",
    "    return retinex_adjust(retinex(nimg))\n",
    "\n",
    "def standard_deviation_weighted_grey_world(nimg,subwidth,subheight):\n",
    "    \"\"\"\n",
    "    This function does not work correctly\n",
    "    \"\"\"\n",
    "    nimg = nimg.astype(np.uint32)\n",
    "    height, width,ch = nimg.shape\n",
    "    strides = nimg.itemsize*np.array([width*subheight,subwidth,width,3,1])\n",
    "    shape = (height/subheight, width/subwidth, subheight, subwidth,3)\n",
    "    blocks = np.lib.stride_tricks.as_strided(nimg, shape=shape, strides=strides)\n",
    "    y,x = blocks.shape[:2]\n",
    "    std_r = np.zeros([y,x],dtype=np.float16)\n",
    "    std_g = np.zeros([y,x],dtype=np.float16)\n",
    "    std_b = np.zeros([y,x],dtype=np.float16)\n",
    "    std_r_sum = 0.0\n",
    "    std_g_sum = 0.0\n",
    "    std_b_sum = 0.0\n",
    "    for i in xrange(y):\n",
    "        for j in xrange(x):\n",
    "            subblock = blocks[i,j]\n",
    "            subb = subblock.transpose(2, 0, 1)\n",
    "            std_r[i,j]=np.std(subb[0])\n",
    "            std_g[i,j]=np.std(subb[1])\n",
    "            std_b[i,j]=np.std(subb[2])\n",
    "            std_r_sum += std_r[i,j]\n",
    "            std_g_sum += std_g[i,j]\n",
    "            std_b_sum += std_b[i,j]\n",
    "    sdwa_r = 0.0\n",
    "    sdwa_g = 0.0\n",
    "    sdwa_b = 0.0\n",
    "    for i in xrange(y):\n",
    "        for j in xrange(x):\n",
    "            subblock = blocks[i,j]\n",
    "            subb = subblock.transpose(2, 0, 1)\n",
    "            mean_r=np.mean(subb[0])\n",
    "            mean_g=np.mean(subb[1])\n",
    "            mean_b=np.mean(subb[2])\n",
    "            sdwa_r += (std_r[i,j]/std_r_sum)*mean_r\n",
    "            sdwa_g += (std_g[i,j]/std_g_sum)*mean_g\n",
    "            sdwa_b += (std_b[i,j]/std_b_sum)*mean_b\n",
    "    sdwa_avg = (sdwa_r+sdwa_g+sdwa_b)/3\n",
    "    gain_r = sdwa_avg/sdwa_r\n",
    "    gain_g = sdwa_avg/sdwa_g\n",
    "    gain_b = sdwa_avg/sdwa_b\n",
    "    nimg = nimg.transpose(2, 0, 1)\n",
    "    nimg[0] = np.minimum(nimg[0]*gain_r,255)\n",
    "    nimg[1] = np.minimum(nimg[1]*gain_g,255)\n",
    "    nimg[2] = np.minimum(nimg[2]*gain_b,255)\n",
    "    return nimg.transpose(1, 2, 0).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjgnobcOkegm"
   },
   "outputs": [],
   "source": [
    "# Final Feature Extraction Algo\n",
    "# See if there any room for improvements\n",
    "# See if colour consistency make any changes for real\n",
    "def FeatureExtraction(img):\n",
    "    pil_image=to_pil(retinex((img)))\n",
    "    open_cv_image = np.array(pil_image) \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "    hist_equalization_result = HistogramEqualization(open_cv_image)\n",
    "    edgeEnhanced=edgeEnhancement(hist_equalization_result)\n",
    "    gray= Convert2Grayscale(edgeEnhanced)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cDiX9m0keeu"
   },
   "outputs": [],
   "source": [
    "def ProposedSegmentationAlgoWithDisplay(path): \n",
    "    img=loadSampleImg(path)\n",
    "    pil_image=to_pil(retinex((img)))\n",
    "    open_cv_image = np.array(pil_image) \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "    hist_equalization_result = HistogramEqualization(open_cv_image)\n",
    "    edgeEnhanced=edgeEnhancement(hist_equalization_result)\n",
    "    gray= Convert2Grayscale(edgeEnhanced)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4)\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(hist_equalization_result)\n",
    "    ax[2].imshow(edgeEnhanced)\n",
    "    ax[3].imshow(gray)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DDb4VDskebk"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rBrightness(image,ratio):\n",
    "    hsv=cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    brightness=np.float64(hsv[:, :, 2])\n",
    "    brightness=brightness*(1.0+np.random.uniform(-ratio,ratio))\n",
    "    brightness[brightness>255]=255\n",
    "    brightness[brightness<0]=0\n",
    "    hsv[:, :, 2]=brightness\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "def rRotation(image, angle):\n",
    "    if angle==0:\n",
    "        return image\n",
    "    angle=np.random.uniform(-angle,angle)\n",
    "    rows,cols=image.shape[:2]\n",
    "    size=rows,cols\n",
    "    center=rows/2,cols/2\n",
    "    scale=1.0\n",
    "    rotation=cv2.getRotationMatrix2D(center,angle,scale)\n",
    "    return cv2.warpAffine(image,rotation,size)\n",
    "def rTranslation(image, translation):\n",
    "    if translation==0:\n",
    "        return 0\n",
    "    rows,cols=image.shape[:2]\n",
    "    size=rows,cols\n",
    "    x=np.random.uniform(-translation,translation)\n",
    "    y=np.random.uniform(-translation,translation)\n",
    "    trans=np.float32([[1,0,x],[0,1,y]])\n",
    "    return cv2.warpAffine(image,trans,size)\n",
    "def rShear(image, shear):\n",
    "    if shear==0:\n",
    "        return image\n",
    "    rows,cols=image.shape[:2]\n",
    "    size=rows,cols\n",
    "    left,right,top,bottom=shear,cols-shear,shear,rows-shear\n",
    "    dx=np.random.uniform(-shear,shear)\n",
    "    dy=np.random.uniform(-shear,shear)\n",
    "    p1=np.float32([[left,top],[right,top],[left,bottom]])\n",
    "    p2=np.float32([[left+dx,top],[right+dx,top+dy],[left,bottom+dy]])\n",
    "    move=cv2.getAffineTransform(p1,p2)\n",
    "    return cv2.warpAffine(image,move,size)\n",
    "def agument_image(image,brightness,angle,translation,shear):\n",
    "    image=rBrightness(image,brightness)\n",
    "    image=rRotation(image,angle)\n",
    "    image=rTranslation(image,translation)\n",
    "    image=rShear(image,shear)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QY8c6AjpwAu"
   },
   "outputs": [],
   "source": [
    "test_dir='/content/drive/My Drive/DL_PRJ/Traffic_Sign_Recognition_Detection/Source_Code/DataSet/Testing_DataSet/GTSRB/Final_Test/Images/'\n",
    "labels_dir='/content/drive/My Drive/DL_PRJ/Traffic_Sign_Recognition_Detection/Source_Code/DataSet/Testing_DataSet/GT-final_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNfzZC9ZpelE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxDfzBRkpqvM"
   },
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiwN-68IkeX6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_test_image():\n",
    "  test_image_array=[]\n",
    "  test_image_labels=[]\n",
    "  test_Data_frame = pd.read_csv(labels_dir, delimiter=';')\n",
    "  for _dir in os.listdir(test_dir):\n",
    "    if _dir=='GT-final_test.test.csv':\n",
    "      continue\n",
    "    print(\"Directory: \", _dir)\n",
    "    list_=test_Data_frame[test_Data_frame['Filename'] == _dir].index.tolist()\n",
    "    dir_img_label=test_Data_frame.iloc[list_[0]]['ClassId']\n",
    "    # label_df=pd.DataFrame([dir_img_label])\n",
    "    # class_id=dir_img_label['ClassId']\n",
    "    # print(dir_img_label)\n",
    "\n",
    "    # img_path=os.path.join(_dir)\n",
    "    # print(img_path)\n",
    "    img = readImage(test_dir+_dir)\n",
    "    img_features = np.array(FeatureExtraction(img))[:,:,np.newaxis]\n",
    "    test_image_array.append(img_features)\n",
    "    test_image_labels.append(dir_img_label)\n",
    "  return   test_image_array, test_image_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZTpCaEckeQz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7ofWyTekeLC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OBdhPgEkeIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDGH8RAGjrx-"
   },
   "outputs": [],
   "source": [
    "## Function to get csv path\n",
    "def getCsvPath(train_path_dir):\n",
    "     return os.path.join(train_path_dir,list(filter(lambda x: '.csv' in x, os.listdir(train_path_dir)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuKTFq5Tjro0"
   },
   "outputs": [],
   "source": [
    "# Function to get category directory and corresponding data frame\n",
    "def getDirAndDataFrame(_dir):\n",
    "    train_category_dir= os.path.join(training_dataset_dir,_dir)\n",
    "    train_csv_path= getCsvPath(train_category_dir)\n",
    "    train_Data_frame = pd.read_csv(train_csv_path, delimiter=';')\n",
    "    return train_category_dir, train_Data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9Jpicd-jr0A"
   },
   "outputs": [],
   "source": [
    "## Read image in path\n",
    "def readImage(path):\n",
    "    img= cv2.imread(path)\n",
    "    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img= cv2.resize(img,(80,80))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NWiV-Wnjr9X"
   },
   "outputs": [],
   "source": [
    "def loadSampleImg(path):\n",
    "    img=readImage(path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYDBzK76j6xL"
   },
   "outputs": [],
   "source": [
    "## Training DataSet Directory\n",
    "training_dataset_dir = '/content/drive/My Drive/DL_PRJ/Traffic_Sign_Recognition_Detection/Source_Code/DataSet/Training_DataSet/GTSRB/Final_Training/Images'\n",
    "\n",
    "def LoadTrainDataSet():\n",
    "    train_image_array=[]\n",
    "    train_labels_array=[]\n",
    "\n",
    "    for _dir in os.listdir(training_dataset_dir):\n",
    "        print(\"Directory: \", _dir)\n",
    "        # Get Directory for train with certain class ID and its associated dataframe\n",
    "        train_category_dir, train_data_frame= getDirAndDataFrame(_dir)\n",
    "        # Get ClassID for entire category\n",
    "        dir_img_label=train_data_frame.iloc[0]['ClassId']\n",
    "\n",
    "        for img_path in glob.glob(os.path.join(train_category_dir, '*.ppm')):\n",
    "\n",
    "            #Load image in category path directory + Extraction of features\n",
    "            img = readImage(img_path)\n",
    "\n",
    "            img_features = np.array(FeatureExtraction(img))[:,:,np.newaxis]\n",
    "            train_image_array.append(img_features)\n",
    "            #Populate associated labels\n",
    "            train_labels_array.append(dir_img_label)\n",
    "            #### for agumentation \n",
    "            random_number_probability=random.uniform(0, 1)\n",
    "            if random_number_probability>=0.5:\n",
    "              agumented_imgae=agument_image(img,0.7,11,5,2)\n",
    "              img_features_= np.array(FeatureExtraction(agumented_imgae))[:,:,np.newaxis]\n",
    "              train_labels_array.append(dir_img_label)\n",
    "              train_image_array.append(img_features_)\n",
    "            \n",
    "    train_image_array=np.stack(train_image_array, axis=0)\n",
    "    return train_image_array, train_labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ql68WrJ4h-cu",
    "outputId": "e60a25d5-32e8-4ca0-9045-51dffe92c29e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "T90L1Lajd4Su",
    "outputId": "d1902eac-ccba-4e82-9df9-2ae0d9f60ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "/device:GPU:0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2kvlhNHd4S3"
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMN-2ghid4S6"
   },
   "outputs": [],
   "source": [
    "class CNN_Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.conv_layer_1 = tf.keras.layers.Conv2D(filter_size,kernel_1, strides = 1, padding='same',activation = tf.nn.relu)\n",
    "        self.conv_layer_2 = tf.keras.layers.Conv2D(64,kernel_size=(3, 3), strides = 1, padding='same',activation = tf.nn.relu)\n",
    "        self.pool_layer_2 = tf.keras.layers.MaxPool2D(pooling)\n",
    "        self.drop_2 =       tf.keras.layers.Dropout(dropout)\n",
    "        self.conv_layer_3 = tf.keras.layers.Conv2D(64,kernel_2, strides = 1, padding='same', activation = tf.nn.relu)\n",
    "        self.pool_layer_3 = tf.keras.layers.MaxPool2D(pooling)\n",
    "        self.drop_3 =       tf.keras.layers.Dropout(dropout)\n",
    "        self.conv_layer_4 = tf.keras.layers.Conv2D(filter_size,kernel_2, strides = 1, padding='same', activation = tf.nn.relu)\n",
    "        self.pool_layer_4 = tf.keras.layers.MaxPool2D(pooling)\n",
    "        # self.drop_4 =       tf.keras.layers.Dropout(dropout)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(hidden_layer, activation = tf.nn.relu)\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.final = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv_layer_1(input_tensor)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.pool_layer_2(x)\n",
    "        x=  self.drop_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.pool_layer_3(x)  \n",
    "        x=  self.drop_3(x)      \n",
    "        x = self.conv_layer_4(x)\n",
    "        # x = self.pool_layer_4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LP_wRqmd4S_"
   },
   "outputs": [],
   "source": [
    "def train(TrainingDataSet, ValidationDataSet, model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):  \n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            # Training DataSet\n",
    "            for x_np, y_np in TrainingDataSet:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in ValidationDataSet:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4D72YMTd4TD"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters tuning\n",
    "kernel_1 = (5,5)\n",
    "kernel_2 = (3,3)\n",
    "pooling = (2,2)\n",
    "filter_size = 32\n",
    "hidden_layer = 64\n",
    "dropout = 0.2\n",
    "num_classes = 43\n",
    "\n",
    "# Configurations of batch printing\n",
    "print_every = 1000\n",
    "num_epochs = 20\n",
    "learning_rate = 1.5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aez1sHlzd4TG"
   },
   "outputs": [],
   "source": [
    "model = CNN_Model()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CNN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0n11Qkkpd4TJ"
   },
   "outputs": [],
   "source": [
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.Adam(learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLVkSSJDd4TM"
   },
   "outputs": [],
   "source": [
    "def saveCNNModel():\n",
    "    model.save('cnn_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "AQjqMbCId4To",
    "outputId": "be676f1f-f3ff-4db7-986d-b172cbf7bc47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory:  00042\n",
      "Directory:  00041\n",
      "Directory:  00039\n",
      "Directory:  00038\n",
      "Directory:  00037\n",
      "Directory:  00040\n",
      "Directory:  00035\n"
     ]
    }
   ],
   "source": [
    "DataSetImages, DataSetLabels = LoadTrainDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "B6HJd-KZkeSm",
    "outputId": "a3292f92-84c4-4470-87ff-2c9f10eb24db"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-450b497acefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_image_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_test_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'read_test_image' is not defined"
     ]
    }
   ],
   "source": [
    "test_image_array, test_image_labels=read_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "k2whHKWdd4Tw",
    "outputId": "20b5629b-b025-4a45-881a-1892709dcfe0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-fb906ba7ea7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSetImages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataSetLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataSetLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DataSetImages' is not defined"
     ]
    }
   ],
   "source": [
    "image_train, image_valid,label_train, label_valid = train_test_split(DataSetImages,DataSetLabels,stratify=DataSetLabels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a50gBtkSd4T2"
   },
   "outputs": [],
   "source": [
    "print(image_train.shape)\n",
    "TrainingDataSet= Dataset(image_train,np.array(label_train), batch_size=64, shuffle=True)\n",
    "ValidationDataSet= Dataset(image_valid,np.array(label_valid), batch_size=64, shuffle=True)\n",
    "for t, (x, y) in enumerate(TrainingDataSet):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEpLP5Kod4T7"
   },
   "outputs": [],
   "source": [
    "train(TrainingDataSet, ValidationDataSet, model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-bBuqtOd4UB"
   },
   "outputs": [],
   "source": [
    "model._set_inputs(image_train,np.array(label_train))\n",
    "saveCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pF2s8Hy2-ihZ"
   },
   "outputs": [],
   "source": [
    "saves_model_path='/content/drive/My Drive/DL_PRJ/Traffic_Sign_Recognition_Detection/Source_Code/Model/cnn_model.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_DZeP-W9lRW"
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "  Model = tf.keras.models.load_model(saves_model_path)\n",
    "  return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYZIxBKI-yQw"
   },
   "outputs": [],
   "source": [
    "model= load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dy26ShXj_E-S"
   },
   "outputs": [],
   "source": [
    "prediction_y=model.predict(test_image_array)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Model_Modified.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
