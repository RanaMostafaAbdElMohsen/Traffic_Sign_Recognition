{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition Model\n",
    "## Environment Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntialiseEnv():\n",
    "    nb_dir = os.path.split(os.getcwd())[0]\n",
    "    if nb_dir not in sys.path:\n",
    "        sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir(\"/content/drive/\")\n",
    "!ls\n",
    "import os\n",
    "os.chdir(\"My Drive/[ Masters ] - Deep Learning Proj/Model\")\n",
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "IntialiseEnv()\n",
    "import import_ipynb\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Data_Preparation.Data_Preparation_Croatian import LoadCroatianTrainDataSet\n",
    "from Data_Preparation.Data_Preparation_Croatian import LoadCroatianTestDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU/ CPU Configuration \n",
    "Tensorflow version 2.0\n",
    "Prompt to user if CPU/ GPU is in use with device name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "/device:GPU:0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Use GPU/CPU Configurations\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadTrainDataSet():\n",
    "    print(\"Reading Train Pre-processed DataSet\")\n",
    "    processedTrainDataSet = '../DataSet/Processed_DataSet/CroatianTrainDataSet.pkl'\n",
    "    isProcessedTrainDataSetExits= os.path.exists(processedTrainDataSet)\n",
    "    train_image_array, train_image_labels = None, None\n",
    "    \n",
    "    if isProcessedTrainDataSetExits:\n",
    "        print(\"Loading Processed Train DataSet from Processed_DataSet/CroatianTrainDataSet.pkl\")\n",
    "        file = open(processedTrainDataSet, 'rb')\n",
    "        train_image_array, train_image_labels = pickle.load(file)\n",
    "        file.close()\n",
    "        print(\"Done Loading Train DataSet.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Processed_DataSet/CroatianTrainDataSet.pkl file does not exist\")\n",
    "        print(\"Loading Train DataSet ... This may take a while.\")\n",
    "        train_image_array, train_image_labels =  LoadCroatianTrainDataSet()\n",
    "        file = open(processedTrainDataSet, 'wb')\n",
    "        pickle.dump((train_image_array, train_image_labels), file, protocol=4)\n",
    "        file.close()\n",
    "        print(\"Saving pre-processed train DataSets in Processed_DataSet/CroatianTrainDataSet.pkl\")\n",
    "    return train_image_array, train_image_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadTestDataSet():\n",
    "    processedTestDataSet = '../DataSet/Processed_DataSet/CroatianTestDataSet.pkl'\n",
    "    isProcessedTestDataSetExits= os.path.exists(processedTestDataSet)\n",
    "    test_image_array, test_image_labels = None, None\n",
    "    \n",
    "    if isProcessedTestDataSetExits:\n",
    "        print(\"Loading Processed Test DataSet from Processed_DataSet/CroatianTestDataSet.pkl\")\n",
    "        file = open(processedTestDataSet, 'rb')\n",
    "        test_image_array, test_image_labels = pickle.load(file)\n",
    "        file.close()\n",
    "        print(\"Done Loading Test DataSet.\")\n",
    "    else:\n",
    "        print(\"Processed_DataSet/CroatianTestDataSet.pkl file does not exist\")\n",
    "        print(\"Loading Test DataSet ... This may take a while.\")\n",
    "        test_image_array, test_image_labels = LoadCroatianTestDataSet()\n",
    "        file = open(processedTestDataSet, 'wb')\n",
    "        pickle.dump((test_image_array, test_image_labels), file)\n",
    "        file.close()\n",
    "        print(\"Saving pre-processed test DataSets in Processed_DataSet/CroatianTestDataSet.pkl\")\n",
    "    return test_image_array, test_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Train Pre-processed DataSet\n",
      "Processed_DataSet/CroatianTrainDataSet.pkl file does not exist\n",
      "Loading Train DataSet ... This may take a while.\n",
      "Directory:  00000\n",
      "Directory:  00001\n",
      "Directory:  00002\n",
      "Directory:  00003\n",
      "Directory:  00004\n",
      "Directory:  00005\n",
      "Directory:  00006\n",
      "Directory:  00007\n",
      "Directory:  00008\n",
      "Directory:  00009\n",
      "Directory:  00010\n",
      "Directory:  00011\n",
      "Directory:  00012\n",
      "Directory:  00013\n",
      "Directory:  00014\n",
      "Directory:  00015\n",
      "Directory:  00016\n",
      "Directory:  00017\n",
      "Directory:  00018\n",
      "Directory:  00019\n",
      "Directory:  00020\n",
      "Directory:  00021\n",
      "Directory:  00022\n",
      "Directory:  00023\n",
      "Directory:  00024\n",
      "Directory:  00025\n",
      "Directory:  00026\n",
      "Directory:  00027\n",
      "Directory:  00028\n",
      "Directory:  00029\n",
      "Directory:  00030\n",
      "Saving pre-processed train DataSets in Processed_DataSet/CroatianTrainDataSet.pkl\n"
     ]
    }
   ],
   "source": [
    "train_image_array, train_image_labels= ReadTrainDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4044, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_image_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed_DataSet/CroatianTestDataSet.pkl file does not exist\n",
      "Loading Test DataSet ... This may take a while.\n",
      "Directory:  00000\n",
      "Directory:  00001\n",
      "Directory:  00002\n",
      "Directory:  00003\n",
      "Directory:  00004\n",
      "Directory:  00005\n",
      "Directory:  00006\n",
      "Directory:  00007\n",
      "Directory:  00008\n",
      "Directory:  00009\n",
      "Directory:  00010\n",
      "Directory:  00011\n",
      "Directory:  00012\n",
      "Directory:  00013\n",
      "Directory:  00014\n",
      "Directory:  00015\n",
      "Directory:  00016\n",
      "Directory:  00017\n",
      "Directory:  00018\n",
      "Directory:  00019\n",
      "Directory:  00020\n",
      "Directory:  00021\n",
      "Directory:  00022\n",
      "Directory:  00023\n",
      "Directory:  00024\n",
      "Directory:  00025\n",
      "Directory:  00026\n",
      "Directory:  00027\n",
      "Directory:  00028\n",
      "Directory:  00029\n",
      "Directory:  00030\n",
      "Saving pre-processed test DataSets in Processed_DataSet/CroatianTestDataSet.pkl\n"
     ]
    }
   ],
   "source": [
    "test_image_array, test_image_labels= ReadTestDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1784, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_image_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Split \n",
    "Split Train images dataset into two splits: training and validation respectively 90-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train, image_valid,label_train, label_valid = train_test_split(train_image_array, train_image_labels,stratify=train_image_labels,test_size=0.1,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_image_array\n",
    "del train_image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters tuning\n",
    "kernel_2 = (3,3)\n",
    "pooling = (2,2)\n",
    "dropout = 0.2\n",
    "num_classes = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPool2D, Add,AveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, SeparableConv2D,BatchNormalization,Dropout,MaxPool2D,Flatten,Dense\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay=1E-4\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def model_():\n",
    "    model = None\n",
    "    tf.initializers.Orthogonal(gain=1.0, seed=None)\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv2D(32,kernel_2, input_shape=(60,60,1), strides = 1, padding='valid',activation = tf.nn.relu,kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,kernel_2, activation = tf.nn.relu,kernel_regularizer=l2(0.1)))\n",
    "    model.add(MaxPool2D(pooling))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(128,kernel_2, strides = 1, padding='valid', activation = tf.nn.relu,kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,kernel_2, activation = tf.nn.relu,kernel_regularizer=l2(0.1)))\n",
    "    model.add(MaxPool2D(pooling))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256,kernel_2, strides = 1, padding='valid', activation = tf.nn.relu,kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_2, activation = tf.nn.relu,kernel_regularizer=l2(0.1)))\n",
    "    model.add(MaxPool2D(pooling))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation = tf.nn.relu))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = model_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=2,  verbose=1, min_delta=1e-4, min_lr=1e-20)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(featurewise_center=False, \n",
    "                            featurewise_std_normalization=False, \n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.2,\n",
    "                            shear_range=0.1,\n",
    "                            rotation_range=10.,)from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(featurewise_center=False, \n",
    "                            featurewise_std_normalization=False, \n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.2,\n",
    "                            shear_range=0.1,\n",
    "                            rotation_range=10.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=aug.flow(image_train, np.array(label_train), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, epochs=94, shuffle=True, validation_data=(image_valid, np.array(label_valid)),verbose=1,callbacks=[reduce_lr])             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_image_array, np.array(test_image_labels), batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary \n",
    "Number of parameters used in the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Trained .h5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCNNModel(model_name):\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCNNModel('Trained_Models/Croatian_Winning_xx_xx.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._set_inputs(image_train,np.array(label_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Trained .h5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    Model = tf.keras.models.load_model('Trained_Models/Croatian_Winning_xx_xx.h5')\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Avg. Processing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "model.predict(test_image_array)\n",
    "t2=time.time()\n",
    "print(\"Average Processing time: \", ((t2-t1)/12630)*1000, \" ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
